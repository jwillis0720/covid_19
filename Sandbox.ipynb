{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T00:15:54.525297Z",
     "start_time": "2020-04-13T00:15:54.179167Z"
    }
   },
   "outputs": [],
   "source": [
    "import callbacks\n",
    "\n",
    "MASTER_ALL, MASTER_PID, DATE_MAPPER = callbacks.serve_data(ret=True,serve_local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    global MASTER_ALL\n",
    "    global MASTER_PID\n",
    "    global DATE_MAPPER\n",
    "    global KEY_VALUE\n",
    "\n",
    "    if serve_local:\n",
    "        MASTER_ALL = pd.read_pickle('Data/MASTER_ALL.pkl', compression='gzip')\n",
    "        MASTER_PID = pd.read_pickle('Data/MASTER_PID.pkl', compression='gzip')\n",
    "        DATE_MAPPER = pd.DataFrame(MASTER_ALL.index.get_level_values('Date').unique())\n",
    "        KEY_VALUE = dict(zip(list(MASTER_PID.index), list(\n",
    "            MASTER_PID['Text_Confirmed'].str.split('<br>').str.get(0).str.replace('US', 'United States'))))\n",
    "        KEY_VALUE = pd.DataFrame(list(KEY_VALUE.values()), index=KEY_VALUE.keys(), columns=['name'])\n",
    "\n",
    "        if ret:\n",
    "            return MASTER_ALL, MASTER_PID, DATE_MAPPER\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    # COUNTRY\n",
    "    JHU_DF_AGG_COUNTRY = pd.read_csv(\n",
    "        'https://jordansdatabucket.s3-us-west-2.amazonaws.com/covid19data/JHU_DF_AGG_COUNTRY.csv.gz', index_col=0, parse_dates=['Date'])\n",
    "    JHU_DF_AGG_COUNTRY['granularity'] = 'country'\n",
    "    JHU_DF_AGG_COUNTRY['Text_Confirmed'] = JHU_DF_AGG_COUNTRY['forcast'].apply(lambda x: \"\" if not x else \"**Predicted**<br>\") + JHU_DF_AGG_COUNTRY['country'] + \"<br> Total Cases: \" + JHU_DF_AGG_COUNTRY['confirmed'].apply(\n",
    "        lambda x: \"{:,}\".format(int(x)))\n",
    "    JHU_DF_AGG_COUNTRY['Text_Deaths'] = JHU_DF_AGG_COUNTRY['forcast'].apply(lambda x: \"\" if not x else \"**Predicted**<br>\") + JHU_DF_AGG_COUNTRY['country'] + \"<br> Total Deaths: \" + JHU_DF_AGG_COUNTRY['deaths'].apply(\n",
    "        lambda x: \"{:,}\".format(int(x)))\n",
    "\n",
    "    # Province\n",
    "    JHU_DF_AGG_PROVINCE = pd.read_csv(\n",
    "        'https://jordansdatabucket.s3-us-west-2.amazonaws.com/covid19data/JHU_DF_AGG_PROVINCE.csv.gz', index_col=0, parse_dates=['Date'])\n",
    "    JHU_DF_AGG_PROVINCE['granularity'] = 'province'\n",
    "    JHU_DF_AGG_PROVINCE['Text_Confirmed'] = JHU_DF_AGG_PROVINCE['forcast'].apply(lambda x: \"\" if not x else \"**Predicted**<br>\") + JHU_DF_AGG_PROVINCE['province'] + \", \" + JHU_DF_AGG_PROVINCE['country'] + \"<br> Total Cases: \" + JHU_DF_AGG_PROVINCE['confirmed'].apply(\n",
    "        lambda x: \"{:,}\".format(int(x)))\n",
    "    JHU_DF_AGG_PROVINCE['Text_Deaths'] = JHU_DF_AGG_PROVINCE['forcast'].apply(lambda x: \"\" if not x else \"**Predicted**<br>\") + JHU_DF_AGG_PROVINCE['province'] + \", \" + JHU_DF_AGG_PROVINCE['country'] + \"<br> Total Deaths: \" + JHU_DF_AGG_PROVINCE['deaths'].apply(\n",
    "        lambda x: \"{:,}\".format(int(x)))\n",
    "\n",
    "    # State\n",
    "    CSBS_DF_AGG_STATE = pd.read_csv(\n",
    "        'https://jordansdatabucket.s3-us-west-2.amazonaws.com/covid19data/CSBS_DF_AGG_STATE.csv.gz', index_col=0, parse_dates=['Date'])\n",
    "    # CSBS_DF_AGG_STATE.rename({'state':'province'},axis=1 ,inplace=True)\n",
    "    CSBS_DF_AGG_STATE['granularity'] = 'state'\n",
    "    CSBS_DF_AGG_STATE['Text_Confirmed'] = CSBS_DF_AGG_STATE['forcast'].apply(lambda x: \"\" if not x else \"**Predicted**<br>\") + CSBS_DF_AGG_STATE['state'] + \", \" + CSBS_DF_AGG_STATE['country'] + \"<br> Total Cases: \" + CSBS_DF_AGG_STATE['confirmed'].apply(\n",
    "        lambda x: \"{:,}\".format(int(x)))\n",
    "    CSBS_DF_AGG_STATE['Text_Deaths'] = CSBS_DF_AGG_STATE['forcast'].apply(lambda x: \"\" if not x else \"**Predicted**<br>\") + CSBS_DF_AGG_STATE['state'] + \", \" + CSBS_DF_AGG_STATE['country'] + \"<br> Total Deaths: \" + CSBS_DF_AGG_STATE['deaths'].apply(\n",
    "        lambda x: \"{:,}\".format(int(x)))\n",
    "\n",
    "    # County\n",
    "    CSBS_DF_AGG_COUNTY = pd.read_csv(\n",
    "        'https://jordansdatabucket.s3-us-west-2.amazonaws.com/covid19data/CSBS_DF_AGG_COUNTY.csv.gz', index_col=0, parse_dates=['Date'])\n",
    "    CSBS_DF_AGG_COUNTY['granularity'] = 'county'\n",
    "    CSBS_DF_AGG_COUNTY['Text_Confirmed'] = CSBS_DF_AGG_COUNTY['forcast'].apply(lambda x: \"\" if not x else \"**Predicted**<br>\") + CSBS_DF_AGG_COUNTY['county'] + \", \" + CSBS_DF_AGG_COUNTY['province'] + \"<br> Total Cases: \" + CSBS_DF_AGG_COUNTY['confirmed'].apply(\n",
    "        lambda x: \"{:,}\".format(int(x)))\n",
    "    CSBS_DF_AGG_COUNTY['Text_Deaths'] = CSBS_DF_AGG_COUNTY['forcast'].apply(lambda x: \"\" if not x else \"**Predicted**<br>\") + CSBS_DF_AGG_COUNTY['county'] + \", \" + CSBS_DF_AGG_COUNTY['province'] + \"<br> Total Deaths: \" + CSBS_DF_AGG_COUNTY['deaths'].apply(\n",
    "        lambda x: \"{:,}\".format(int(x)))\n",
    "\n",
    "    # Master df has all our entries with\n",
    "    master_df = pd.concat([JHU_DF_AGG_COUNTRY, JHU_DF_AGG_PROVINCE, CSBS_DF_AGG_STATE, CSBS_DF_AGG_COUNTY])\n",
    "\n",
    "    columns = ['Date', 'country', 'province', 'state', 'county', 'granularity']\n",
    "    master_df = master_df[columns+[i for i in master_df.columns if i not in columns]\n",
    "                          ].reset_index(drop=True).fillna('N/A')\n",
    "\n",
    "    worldwide_df = master_df[master_df['granularity'] == 'country'].groupby(['Date', 'forcast']).sum().reset_index()\n",
    "    worldwide_df['country'] = 'worldwide'\n",
    "    worldwide_df['Text_Confirmed'] = 'Worldwide <br>'\n",
    "    master_df = master_df.append(worldwide_df).reset_index(drop=True)\n",
    "    # For latest date, the dataframes we merged each have a different last reported date (CSBS is faster than JHU), so we have to groupby and ask the minmal date from each gruop\n",
    "    latest_date = master_df[master_df['forcast'] == False].groupby('granularity').tail(1)['Date'].min()\n",
    "\n",
    "    # Master_latest has all our entries in it and thus should be used by everything.\n",
    "    master_latest = master_df[master_df['Date'] == latest_date].reset_index(drop=True)\n",
    "\n",
    "    pid = ['country', 'province', 'state', 'county', 'granularity']\n",
    "    assert(len(master_latest) == len(master_df[master_df['Date'] == latest_date].groupby(pid)))\n",
    "\n",
    "    # Now join the master df with the pID number from the latest\n",
    "    master_pid = master_latest.reset_index().set_index(pid).rename({'index': 'pid'}, axis=1)\n",
    "    master_df = master_df.set_index(pid).join(master_pid['pid']).reset_index().set_index(['pid', 'Date']).sort_index()\n",
    "\n",
    "    # Finally, set the pid back as the index\n",
    "    master_latest = master_pid.reset_index().set_index('pid')\n",
    "\n",
    "    # Master Latest has all the entries at the latest non forcast date. The index is the pID\n",
    "    # Master DF has all the entries with all dates. The index is the pID and Date\n",
    "    MASTER_PID = master_latest\n",
    "    MASTER_ALL = master_df\n",
    "\n",
    "    # Those predictions which are less than 0\n",
    "    MASTER_ALL.loc[MASTER_ALL['confirmed'] < 0, 'confirmed'] = 0.0\n",
    "    MASTER_ALL.loc[MASTER_ALL['deaths'] < 0, 'deaths'] = 0.0\n",
    "\n",
    "    max_size = 1500\n",
    "\n",
    "    bins, ret_bins = pd.qcut(MASTER_ALL[(MASTER_ALL['confirmed'] >= 1) & (MASTER_ALL['country'] != 'worldwide')]['confirmed'], q=[\n",
    "        0, .5, 0.6, 0.70, 0.75, 0.8, 0.85, 0.9, 0.95, 0.999, 1], duplicates='drop', retbins=True)\n",
    "    yellows = [\"#606056\", \"#6e6e56\", \"#7b7c55\", \"#898a54\", \"#979953\",\n",
    "               \"#a5a850\", \"#b4b74d\", \"#c3c649\", \"#d2d643\", \"#e2e53c\"]\n",
    "    reds = [\"#5a4f4f\", \"#704d4d\", \"#854b4a\", \"#994746\", \"#ac4340\",\n",
    "            \"#be3c3a\", \"#cf3531\", \"#e02a27\", \"#f01c19\", \"#ff0000\"]\n",
    "    labels = np.geomspace(1, max_size, num=len(ret_bins)-1)\n",
    "    MASTER_ALL['CSize'] = pd.cut(MASTER_ALL['confirmed'], bins=ret_bins, labels=labels).astype(float).fillna(0)\n",
    "    MASTER_ALL['DSize'] = pd.cut(MASTER_ALL['deaths'], bins=ret_bins, labels=labels).astype(float).fillna(0)\n",
    "    MASTER_ALL['CColor'] = pd.cut(MASTER_ALL['confirmed'], bins=ret_bins,\n",
    "                                  labels=yellows).astype(str).replace({'nan': 'white'})\n",
    "    MASTER_ALL['DColor'] = pd.cut(MASTER_ALL['deaths'], bins=ret_bins,\n",
    "                                  labels=reds).astype(str).replace({'nan': 'white'})\n",
    "    DATE_MAPPER = pd.DataFrame(MASTER_ALL.index.get_level_values('Date').unique())\n",
    "\n",
    "    KEY_VALUE = dict(zip(list(MASTER_PID.index), list(\n",
    "        MASTER_PID['Text_Confirmed'].str.split('<br>').str.get(0).str.replace('US', 'United States'))))\n",
    "    KEY_VALUE = pd.DataFrame(list(KEY_VALUE.values()), index=KEY_VALUE.keys(), columns=['name'])\n",
    "\n",
    "    MASTER_ALL.to_pickle('Data/MASTER_ALL.pkl', compression='gzip')\n",
    "    MASTER_PID.to_pickle('Data/MASTER_PID.pkl', compression='gzip')\n",
    "\n",
    "    if ret:\n",
    "        return MASTER_ALL, MASTER_PID, DATE_MAPPER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/provence_df_per_day.csv.gz',\n",
       " 'Data/csbs_df_Archive_4_9_2020.csv.gz',\n",
       " 'Data/JHU_DF_AGG_PROVINCE.csv.gz',\n",
       " 'Data/csbs_df_Archive_4_5_2020.csv.gz',\n",
       " 'Data/jhu_df_time.csv.gz',\n",
       " 'Data/country_df_per_day.csv.gz',\n",
       " 'Data/Merged_df.csv.gz',\n",
       " 'Data/csbs_df_Archive_4_6_2020.csv.gz',\n",
       " 'Data/csbs_df_Archive_3_28_2020.csv.gz',\n",
       " 'Data/csbs_df_Archive_3_31_2020.csv.gz',\n",
       " 'Data/csbs_df_Archive_03_25_2020.csv.gz',\n",
       " 'Data/CSBS_DF_AGG_COUNTY.csv.gz',\n",
       " 'Data/per_day_stats_by_country.csv.gz',\n",
       " 'Data/csbs_df_Archive_4_3_2020.csv.gz',\n",
       " 'Data/jhu_df.csv.gz',\n",
       " 'Data/csbs_df_Archive_4_4_2020.csv.gz',\n",
       " 'Data/csbs_df_Archive_4_11_2020.csv.gz',\n",
       " 'Data/csbs_df.csv.gz',\n",
       " 'Data/JHU_DF_AGG_COUNTRY.csv.gz',\n",
       " 'Data/csbs_df_Archive_4_8_2020.csv.gz',\n",
       " 'Data/csbs_df_Archive_4_1_2020.csv.gz',\n",
       " 'Data/csbs_df_Archive_3_26_2020.csv.gz',\n",
       " 'Data/csbs_df_Archive_4_2_2020.csv.gz',\n",
       " 'Data/CSBS_DF_AGG_STATE.csv.gz',\n",
       " 'Data/per_day_stats_by_state.csv.gz',\n",
       " 'Data/per_day_stats_by_county.csv.gz',\n",
       " 'Data/csbs_df_Archive_3_30_2020.csv.gz',\n",
       " 'Data/TestJHU_DF_AGG_COUNTRY.csv.gz',\n",
       " 'Data/combined_time_scales.csv.gz',\n",
       " 'Data/csbs_df_Archive_4_7_2020.csv.gz',\n",
       " 'Data/MASTER_ALL.pkl',\n",
       " 'Data/MASTER_PID.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "(glob.glob('Data/*.csv.gz') + glob.glob('Data/*.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
